<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="updatedContent" value="# Full QA Chatbot Agent&#10;&#10;This project is a Question Answering (QA) chatbot agent that leverages Retrieval-Augmented Generation (RAG) to answer user queries based on provided documents. It is built using Python and Streamlit for the user interface.&#10;&#10;## Features&#10;- Interactive chat UI for asking questions&#10;- RAG-based response generation&#10;- Contextual document retrieval&#10;- Easy integration with custom document files&#10;&#10;## Project Structure&#10;```&#10;full-qa-chatbot-agent/&#10;├── requirements.txt&#10;├── files/                  # Directory for source documents (PDFs, etc.)&#10;├── src/&#10;│   └── chatbot/&#10;│       ├── app.py          # Main Streamlit app&#10;│       └── util/&#10;│           └── api_calls.py&#10;└── ...&#10;```&#10;&#10;## Getting Started&#10;&#10;### Prerequisites&#10;- Python 3.10+&#10;- pip&#10;&#10;### Installation&#10;1. Clone the repository:&#10;   ```bash&#10;   git clone &lt;repo-url&gt;&#10;   cd full-qa-chatbot-agent&#10;   ```&#10;2. Install dependencies:&#10;   ```bash&#10;   pip install -r requirements.txt&#10;   ```&#10;&#10;### Running the App&#10;Start the Streamlit app:&#10;```bash&#10;streamlit run src/chatbot/app.py&#10;```&#10;&#10;### Adding Documents&#10;Place your PDF or other supported files in the `files/` directory. The chatbot will use these documents as context for answering questions.&#10;&#10;## Usage&#10;- Open the app in your browser (Streamlit will provide a local URL).&#10;- Enter your question in the chat box and submit.&#10;- The chatbot will respond with an answer and show relevant context from your documents.&#10;&#10;## License&#10;MIT License&#10;&#10;## Author&#10;Danilo De Castro&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/chatbot/app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/chatbot/app.py" />
              <option name="originalContent" value="import os&#10;&#10;import streamlit as st&#10;from dotenv import load_dotenv&#10;from langchain.chains.combine_documents import create_stuff_documents_chain&#10;from langchain.chains.retrieval import create_retrieval_chain&#10;from langchain_community.vectorstores import Chroma&#10;from langchain_core.output_parsers import StrOutputParser&#10;from langchain_core.prompts import ChatPromptTemplate&#10;from langchain_groq import ChatGroq&#10;from langchain_community.document_loaders import PyPDFDirectoryLoader&#10;from langchain.text_splitter import RecursiveCharacterTextSplitter&#10;from langchain_ollama import OllamaEmbeddings&#10;from langchain_openai import OpenAIEmbeddings&#10;&#10;GROQ_MODELS = [&quot;gemma2-9b-it&quot;, &quot;llama-3.1-8b-instant&quot;, &quot;llama-3.3-70b-versatile&quot;,&#10;               &quot;meta-llama/llama-guard-4-12b&quot;, &quot;deepseek-r1-distill-llama-70b&quot;]&#10;&#10;load_dotenv()&#10;&#10;&#10;def create_prompt():&#10;    return ChatPromptTemplate(&#10;        [&#10;            (&quot;system&quot;, &quot;You are a helpful assistant, respond to the user questions&quot;),&#10;            (&quot;human&quot;, &quot;Question:{question}&quot;)&#10;        ]&#10;    )&#10;&#10;&#10;def create_prompt_for_rag():&#10;    return ChatPromptTemplate.from_template(&#10;        &quot;&quot;&quot;&#10;        Answer the questions based on provided context only.&#10;        Context: {context}&#10;        &#10;        Question: {input}&#10;        &quot;&quot;&quot;&#10;    )&#10;&#10;&#10;def get_groc_model(max_tokens, model_id, temperature):&#10;    return ChatGroq(model=model_id, api_key=os.getenv(&quot;GROQ_API_KEY&quot;), temperature=temperature, max_tokens=max_tokens)&#10;&#10;&#10;def generate_response(user_question, model_id, temperature, max_tokens):&#10;    llm = get_groc_model(max_tokens, model_id, temperature)&#10;    chain = create_prompt() | llm | StrOutputParser()&#10;    return chain.invoke({&quot;question&quot;: user_question})&#10;&#10;&#10;def create_vector_embeddings():&#10;    if &quot;vector_store&quot; not in st.session_state:&#10;        st.session_state.embeddings = OpenAIEmbeddings()&#10;        st.session_state.loader = PyPDFDirectoryLoader(&quot;files&quot;)&#10;        st.session_state.docs = st.session_state.loader.load()&#10;        st.session_state.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)&#10;        st.session_state.chunks = st.session_state.text_splitter.split_documents(st.session_state.docs)&#10;        st.session_state.vector_store = Chroma.from_documents(st.session_state.docs, st.session_state.embeddings)&#10;&#10;&#10;def rag_search_ui(model, temp, max_tokens):&#10;    uploader = st.file_uploader(&quot;Upload files&quot;, [&quot;pdf&quot;, &quot;word&quot;, &quot;csv&quot;], accept_multiple_files=True)&#10;    if uploader:&#10;        for uploaded_file in uploader:&#10;            save_path = os.path.join(&quot;files&quot;, uploaded_file.name)&#10;            with open(save_path, &quot;wb&quot;) as f:&#10;                f.write(uploaded_file.getbuffer())&#10;        st.success(&quot;Files uploaded and saved to 'files/' folder.&quot;)&#10;        create_vector_embeddings()&#10;        st.write(&quot;Docs loaded!&quot;)&#10;&#10;    # Use a form to control when the query is submitted&#10;    with st.form(key=&quot;rag_form&quot;, clear_on_submit=True):&#10;        user_prompt = st.text_input(&quot;Enter your question&quot;)&#10;        submit_button = st.form_submit_button(&quot;Search&quot;)&#10;&#10;    if submit_button and user_prompt:&#10;        doc_chain = create_stuff_documents_chain(&#10;            get_groc_model(model_id=model, temperature=temp, max_tokens=max_tokens), prompt=create_prompt_for_rag())&#10;        retriever = st.session_state.vector_store.as_retriever()&#10;        response = create_retrieval_chain(retriever, doc_chain).invoke({&quot;input&quot;: user_prompt})&#10;&#10;        st.session_state.last_response = response&#10;        st.write(response[&quot;answer&quot;])&#10;        with st.expander(&quot;Sources&quot;):&#10;            for index, doc in enumerate(st.session_state.last_response[&quot;context&quot;]):&#10;                st.write(doc)&#10;                st.write('------------')&#10;&#10;&#10;def chat_ui():&#10;    model, selected_max_tokens, selected_temperature = set_base_ui()&#10;    if 'user_input' not in st.session_state:&#10;        st.session_state.user_input = &quot;&quot;&#10;&#10;&#10;    with st.form(key=&quot;rag_form&quot;, clear_on_submit=True):&#10;        user_prompt = st.text_input(&quot;Enter your question&quot;)&#10;        submit_button = st.form_submit_button(&quot;Send&quot;)&#10;&#10;    if submit_button or user_prompt:&#10;        if user_prompt:&#10;            response = generate_response(user_prompt, model_id=model, temperature=selected_temperature,&#10;                                         max_tokens=selected_max_tokens)&#10;            st.write(response)&#10;            st.session_state.user_input = &quot;&quot;&#10;        else:&#10;            st.write(&quot;I did not get you question, did you ask any ?&quot;)&#10;&#10;&#10;def set_base_ui():&#10;    st.title(&quot;Q&amp;A Chatbot&quot;)&#10;    st.sidebar.title(&quot;settings&quot;)&#10;    model = st.sidebar.selectbox(&quot;Select a model&quot;, GROQ_MODELS)&#10;    selected_temperature = st.sidebar.slider(&quot;temperature&quot;, min_value=0.0, max_value=1.0, value=0.7)&#10;    selected_max_tokens = st.sidebar.slider(&quot;Max Tokens&quot;, min_value=50, max_value=300, value=150)&#10;    st.write(&quot;I'm Danilo's Chatbot. How can I help you&quot;)&#10;    return model, selected_max_tokens, selected_temperature&#10;&#10;&#10;def set_rag_ui():&#10;    model, selected_max_tokens, selected_temperature = set_base_ui()&#10;    rag_search_ui(model, selected_temperature, selected_max_tokens)&#10;&#10;&#10;mode = st.sidebar.radio(&quot;Mode&quot;,[&quot;Chat&quot;, &quot;RAG&quot;])&#10;if mode == &quot;RAG&quot;:&#10;    set_rag_ui()&#10;else:&#10;    chat_ui()" />
              <option name="updatedContent" value="import os&#10;&#10;import streamlit as st&#10;from dotenv import load_dotenv&#10;from langchain.chains.combine_documents import create_stuff_documents_chain&#10;from langchain.chains.retrieval import create_retrieval_chain&#10;from langchain_community.vectorstores import Chroma&#10;from langchain_core.output_parsers import StrOutputParser&#10;from langchain_core.prompts import ChatPromptTemplate&#10;from langchain_groq import ChatGroq&#10;from langchain_community.document_loaders import PyPDFDirectoryLoader&#10;from langchain.text_splitter import RecursiveCharacterTextSplitter&#10;from langchain_ollama import OllamaEmbeddings&#10;from langchain_openai import OpenAIEmbeddings&#10;&#10;GROQ_MODELS = [&quot;gemma2-9b-it&quot;, &quot;llama-3.1-8b-instant&quot;, &quot;llama-3.3-70b-versatile&quot;,&#10;               &quot;meta-llama/llama-guard-4-12b&quot;, &quot;deepseek-r1-distill-llama-70b&quot;]&#10;&#10;load_dotenv()&#10;&#10;&#10;def create_prompt():&#10;    return ChatPromptTemplate(&#10;        [&#10;            (&quot;system&quot;, &quot;You are a helpful assistant, respond to the user questions&quot;),&#10;            (&quot;human&quot;, &quot;Question:{question}&quot;)&#10;        ]&#10;    )&#10;&#10;&#10;def create_prompt_for_rag():&#10;    return ChatPromptTemplate.from_template(&#10;        &quot;&quot;&quot;&#10;        Answer the questions based on provided context only.&#10;        Context: {context}&#10;        &#10;        Question: {input}&#10;        &quot;&quot;&quot;&#10;    )&#10;&#10;&#10;def get_groc_model(max_tokens, model_id, temperature):&#10;    return ChatGroq(model=model_id, api_key=os.getenv(&quot;GROQ_API_KEY&quot;), temperature=temperature, max_tokens=max_tokens)&#10;&#10;&#10;def generate_response(user_question, model_id, temperature, max_tokens):&#10;    llm = get_groc_model(max_tokens, model_id, temperature)&#10;    chain = create_prompt() | llm | StrOutputParser()&#10;    return chain.invoke({&quot;question&quot;: user_question})&#10;&#10;&#10;def create_vector_embeddings():&#10;    if &quot;vector_store&quot; not in st.session_state:&#10;        st.session_state.embeddings = OpenAIEmbeddings()&#10;        st.session_state.loader = PyPDFDirectoryLoader(&quot;files&quot;)&#10;        st.session_state.docs = st.session_state.loader.load()&#10;        st.session_state.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)&#10;        st.session_state.chunks = st.session_state.text_splitter.split_documents(st.session_state.docs)&#10;        st.session_state.vector_store = Chroma.from_documents(st.session_state.docs, st.session_state.embeddings)&#10;&#10;&#10;def rag_search_ui(model, temp, max_tokens):&#10;    uploader = st.file_uploader(&quot;Upload files&quot;, [&quot;pdf&quot;, &quot;word&quot;, &quot;csv&quot;], accept_multiple_files=True)&#10;    if uploader:&#10;        for uploaded_file in uploader:&#10;            save_path = os.path.join(&quot;files&quot;, uploaded_file.name)&#10;            with open(save_path, &quot;wb&quot;) as f:&#10;                f.write(uploaded_file.getbuffer())&#10;        st.success(&quot;Files uploaded and saved to 'files/' folder.&quot;)&#10;        create_vector_embeddings()&#10;        st.write(&quot;Docs loaded!&quot;)&#10;&#10;    # Use a form to control when the query is submitted&#10;    with st.form(key=&quot;rag_form&quot;, clear_on_submit=True):&#10;        user_prompt = st.text_input(&quot;Enter your question&quot;)&#10;        submit_button = st.form_submit_button(&quot;Search&quot;)&#10;&#10;    if submit_button and user_prompt:&#10;        doc_chain = create_stuff_documents_chain(&#10;            get_groc_model(model_id=model, temperature=temp, max_tokens=max_tokens), prompt=create_prompt_for_rag())&#10;        retriever = st.session_state.vector_store.as_retriever()&#10;        response = create_retrieval_chain(retriever, doc_chain).invoke({&quot;input&quot;: user_prompt})&#10;&#10;        st.session_state.last_response = response&#10;        st.write(response[&quot;answer&quot;])&#10;        with st.expander(&quot;Sources&quot;):&#10;            for index, doc in enumerate(st.session_state.last_response[&quot;context&quot;]):&#10;                st.write(doc)&#10;                st.write('------------')&#10;&#10;&#10;def chat_ui():&#10;    model, selected_max_tokens, selected_temperature = set_base_ui()&#10;    if 'user_input' not in st.session_state:&#10;        st.session_state.user_input = &quot;&quot;&#10;&#10;&#10;    with st.form(key=&quot;rag_form&quot;, clear_on_submit=True):&#10;        user_prompt = st.text_input(&quot;Enter your question&quot;)&#10;        submit_button = st.form_submit_button(&quot;Send&quot;)&#10;&#10;    if submit_button or user_prompt:&#10;        if user_prompt:&#10;            response = generate_response(user_prompt, model_id=model, temperature=selected_temperature,&#10;                                         max_tokens=selected_max_tokens)&#10;            st.write(response)&#10;            st.session_state.user_input = &quot;&quot;&#10;        else:&#10;            st.write(&quot;I did not get you question, did you ask any ?&quot;)&#10;&#10;&#10;def set_base_ui():&#10;    st.title(&quot;Q&amp;A Chatbot&quot;)&#10;    st.sidebar.title(&quot;settings&quot;)&#10;    model = st.sidebar.selectbox(&quot;Select a model&quot;, GROQ_MODELS)&#10;    selected_temperature = st.sidebar.slider(&quot;temperature&quot;, min_value=0.0, max_value=1.0, value=0.7)&#10;    selected_max_tokens = st.sidebar.slider(&quot;Max Tokens&quot;, min_value=50, max_value=300, value=150)&#10;    st.write(&quot;I'm Danilo's Chatbot. How can I help you&quot;)&#10;    return model, selected_max_tokens, selected_temperature&#10;&#10;&#10;def set_rag_ui():&#10;    model, selected_max_tokens, selected_temperature = set_base_ui()&#10;    rag_search_ui(model, selected_temperature, selected_max_tokens)&#10;&#10;&#10;mode = st.sidebar.radio(&quot;Mode&quot;,[&quot;Chat&quot;, &quot;RAG&quot;])&#10;if mode == &quot;RAG&quot;:&#10;    set_rag_ui()&#10;else:&#10;    chat_ui()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>